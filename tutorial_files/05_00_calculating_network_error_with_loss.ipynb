{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Network Error with Loss\n",
    "\n",
    "We need to way to find how far away from the \"truth\" we are. For this we are going to use cross-entropy loss calculation. We will be using one-hot outputs for testing - that is, only one of the items in the list will be 1, the rest will be zero. You could say, the list has \"one-hot\" value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example output from model\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# example target\n",
    "target_output = [1,0,0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "        math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2]\n",
    "        )\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the output is a one-hot output only one of the values actually is going to be valid. the rest will be zeros. Because of this we can just use the expected indices instead to pull the the confidence value - see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "# the index that we expect to be positive.\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "# Print out confidence in the correct output\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing with numpys\n",
    "softmax_outputs = np.array(\n",
    "    [[0.7, 0.1, 0.2],\n",
    "     [0.1, 0.5, 0.4],\n",
    "     [0.02, 0.9, 0.08]])\n",
    "\n",
    "# the index that we expect to be positive.\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes can either be one-hot like we talked about earlier or they could be numberically classed like we did above - IE: `[0,0,1]` or `2`. Here we'll do the same as above, but with one-hot values instead of categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(\n",
    "    [[0.7, 0.1, 0.2],\n",
    "     [0.1, 0.5, 0.4],\n",
    "     [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1,0,0],\n",
    "                          [0,1,0],\n",
    "                          [0,1,0]])\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "     range(len(softmax_outputs)),\n",
    "     class_targets]\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "     softmax_outputs*class_targets,\n",
    "     axis=1)\n",
    "    \n",
    "print(correct_confidences)\n",
    "neg_log = -np.log(correct_confidences)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that we are going to run into is if the model has 100% confidence in a value - we will then be calculated the log of 0 `log(0)` and that results in an error. We can solve this issue by clipping the values to a given range - a very small small value, but not zero, will not cause the failure. We can apply this using numpy's `clip` function. \n",
    "```python\n",
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-f6e7c0610b57>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.11809565095832"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
